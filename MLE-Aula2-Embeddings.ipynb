{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976dc149-2169-4b33-9800-18735db6f5d7",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde8452-56ba-46ea-85d8-ad330254eee2",
   "metadata": {},
   "source": [
    "Olá, turma! \n",
    "\n",
    "Nesta aula, discutiremos um tipo de representação vetorial que lida com os problemas das representações que vimos até agora. Este tipo de representação consegue capturar relações semânticas bem como lida melhor com o contexto de uma palavra numa frase. Assim, uma mesma palavra usada em contextos diferentes consegue ser representada pelo mesmo vetor.\n",
    "\n",
    "Nesta aula, vamos aprender sobre embeddings, como cria-los e como usá-los em algoritmos de machine learning. Acompanhe pelo jupyter notebook a execução dos códigos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865d28b-79b1-4b61-979e-57ac01d5403a",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nas representações que vimos até aqui, descobrimos várias fraquezas que fazem com que esses modelos se tornem impraticáveis de serem usados em grandes conjuntos de dados. \n",
    "\n",
    "1. Elas são representações discretas, ou seja, tratam as unidades de linguagem como unidades atômicas, o que as impede de capturar relações entre palavras.\n",
    "\n",
    "2. Os vetores de características são representações esparsas e de alta dimensionalidade. Além de prejudicar a eficiência computacional, conforme o tamanho do vocabulário aumenta, a dimensionalidade também aumenta, com a maioria dos valores sendo zero para qualquer vetor, o que prejudica o aprendizado.\n",
    "\n",
    "3. Não podem lidar com palavras que estão fora do vocabulário.\n",
    "\n",
    "Para resolver esse problema, métodos para aprender uma representação de baixa dimensão foram sugeridos. ​\n",
    "Para entendermos o conceito dessas representações, também chamadas de Representações Distribuídas, precisamos entender alguns pontos antes. \n",
    "\n",
    "O primeiro deles é o de **Similaridade Distribucional**, que representa a ideia de que o significado de uma palavra pode ser entendido a partir do contexto em que aparece. Isto é conhecido também como conotação, ou seja, o significado é definido pelo contexto. Diferente de denotação, que é o significado literal de uma palavra. \n",
    "\n",
    "O segundo ponto é a **Hipótese Distribucional**, que diz que palavras que aparecem em contextos similares possuem significados similares. Assim, se palavras podem ser representadas por vetores, então duas palavras que aparecem em contextos similares devem possuir vetores similares. \n",
    "\n",
    "O terceiro refere-se à **Representação Distribucional**, que são os vetores de representação que vimos até o presente momento, caracterizados pela alta dimensão e grande esparsidade. \n",
    "\n",
    "Por fim, todos esses conceitos nos levam à **Representação Distribuída**, que são vetores compactos (baixa dimensão) e densos (não-esparsos). Daqui, surgiu o conceito de Word Embeddings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf3e95-4540-458a-9666-50de22ae0c8f",
   "metadata": {},
   "source": [
    "## Motivação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307832e-110e-4183-bd94-98d278b62ac5",
   "metadata": {},
   "source": [
    "A ideia por de trás de Word Embeddings é que é possível representar uma palavra usando um vetor compacto e denso que preserve sua conotação, ou seja, seu significado inferido a partir de um contexto. \n",
    "\n",
    "\n",
    "Segundo o dicionário, significado pode ser essas três coisas:\n",
    "\n",
    "1. Definição atribuída a um termo, palavra, frase ou texto; acepção\n",
    "2. Aquilo que alguma coisa quer dizer; sentido\n",
    "3. Uma ideia que é representada por uma palavra ou frase\n",
    "\n",
    "Logo, percebemos que as representações usadas até aqui perdiam exatamente esse tipo de informação, ou seja:\n",
    "\n",
    "1. Havia perda de nuances. Exemplo: sinônimos - apto, bom, expert.\n",
    "2. Novas palavras. Exemplo: \n",
    "3. Como eu não levo em consideração o contexto, é difícil calcular a similaridade entre palavras.\n",
    "\n",
    "Dessa maneira, precisamos de uma representação que consiga capturar tais relações entre diferentes palavras levando em consideração o contexto. Para resolver esse problema, usaremos representações baseadas em similaridade distribucional.\n",
    "\n",
    "Mas como capturar a relação de uma palavra com seus vizinhos e representar isso através de um vetor denso? Vamos fazer uma analogia.\n",
    "\n",
    "Nessa analogia, estamos contabilizando o quanto uma pessoa é tímida, numa escala de 0 a 100. Podemos representar alguém que tenha dado a nota 38 da seguinte maneira:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfecde-6138-495a-a63c-ceffd2f6d1ef",
   "metadata": {},
   "source": [
    "<img src=\"img/pessoa1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a10812-555d-40e4-ad3d-424a15f9f549",
   "metadata": {},
   "source": [
    "Normalizando o range para o intervalo (-1,1), temos o seguinte gráfico:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9eed5-9574-4e9c-a153-d9086a2af809",
   "metadata": {},
   "source": [
    "<img src=\"img/pessoa2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4993ec-a386-40af-8c94-9302172c124c",
   "metadata": {},
   "source": [
    "Podemos adicionar mais um traço de personalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591f3ff-c406-465e-9b33-3ba8b321d662",
   "metadata": {},
   "source": [
    "<img src=\"img/pessoa3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48001fac-4f90-4fd1-836b-62609897b38c",
   "metadata": {},
   "source": [
    "Com esses dois traços de personalidade, podemos traçar um vetor que representa uma determinada pessoa, como por exemplo: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385e3be-7e0c-41c7-9e26-a74c4dc1da2f",
   "metadata": {},
   "source": [
    "<img src=\"img/pessoa4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c166ab4-f2a7-487a-ba20-4473f7c4deba",
   "metadata": {},
   "source": [
    "E podemos, assim, comparar diversas pessoas: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd715da-82c5-4242-8fa7-fa6b7dd3af24",
   "metadata": {},
   "source": [
    "<img src=\"img/pessoa5.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15ddc4-5b82-4dde-aedf-dbadf2f0caa3",
   "metadata": {},
   "source": [
    "Esses vetores podem ser representados em números, cada qual com a informação associada ao traço de personalidade em questão. A pessoa “azul” pode ser representada por esse vetor, por exemplo: $[-0.4, 0.6]$. Já a pessoa “laranja” pode ser representada por esse vetor: $[-0.3, -0.7]$.\n",
    "\n",
    "Com esses vetores numéricos, é possível usar uma métrica de distância, distância do cosseno ou Euclidiana, por exemplo, e verificar o quão similar dois vetores são. Essa é a ideia por trás dos Words Embeddings.\n",
    "\n",
    "A técnica que deu início aos Words Embeddings foi divulgada num paper de 2013, do Google. Essa técnica recebeu o nome de Word2Vec e vamos entender seu funcionamento agora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8611afc-f2b7-4456-a272-88ed20a36535",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1391a02-1c1e-4c14-a01a-ecdf6a456ae2",
   "metadata": {},
   "source": [
    "O que significa dizer que uma representação textual deveria capturar a similaridade distribucional entre palavras? Vamos analisar alguns exemplos. Se eu fornecer a palavra “Brasil”, outras palavras com similaridade distribucional a essa poderiam ser outros países (“Chile”, “Uruguai”, etc.). Se eu forneço a palavra “Bela”, poderia pensar em sinônimos ou antônimos como palavras com similaridade distribucional. Ou seja, o que estamos tentando capturar são palavras que possuem alta probabilidade de aparecerem num mesmo contexto.\n",
    "\n",
    "Ao aprender tais relações semânticas, o Word2Vec garante que a representação aprendida possui baixa dimensionalidade (palavras são representadas por vetores de 50-1000 dimensões) e são densas (a maioria dos valores dos vetores são diferentes de zero). Tais representações tornam as tarefas de modelos de machine learning mais eficientes.\n",
    "\n",
    "Antes de entrarmos nos detalhes de como o Word2Vec consegue capturar tais relações, vamos construir uma intuição de como ele funciona. Dado um corpus de texto, o objetivo é aprender embeddings de cada palavra no corpus de modo que o vetor da palavra no espaço de embeddings melhor captura o significado da palavra. Para isso, Word2Vec usa similaridade distribucional e hipótese distribucional, ou seja, extrai o significado de uma palavra a partir do seu contexto. Assim, se duas palavras geralmente ocorrem em contextos similares, é altamente provável que seus significados sejam também similares.\n",
    "\n",
    "Dessa maneira, o Word2Vec projeta o significado das palavras num espaço vetorial onde palavras com significados similares tendem a serem agrupadas juntas e palavras com significados muito diferentes estão longe umas das outras.\n",
    "\n",
    "Conceitualmente, o que queremos saber é, dada uma palavra  e as palavras que aparecem em seu contexto , como encontramos um vetor que melhor representa o significado da palavra? Bom, para cada palavra  no corpus, iniciamos um vetor  com valores aleatórios. O modelo Word2Vec refina os valores predizendo  dados os vetores de palavras no contexto . Isto é feito através de uma rede neural de duas camadas, mas antes de construir a rede neural de duas camadas, vamos ver modelos pré-treinados. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880bc52-c098-4965-90cb-da15fe11f448",
   "metadata": {},
   "source": [
    "### Word Embeddings Pré-treinados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58611788-5aae-4840-9bfc-9132a714435c",
   "metadata": {},
   "source": [
    "Treinar seu próprio embedding é uma tarefa muito custosa, tanto em termos de tempo quanto computacionais. Entretanto, para muitos cenários, não é necessário treinar seu próprio embedding, pois muitos embeddings pré-treinados serão suficientes.\n",
    "\n",
    "Um embedding pré-treinado nada mais é que o processo de treinar um embedding num grande corpus de dados e disponibilizar os vetores na internet. Assim, eles podem ser baixados e usados em várias aplicações que você deseja. Tais embeddings podem ser entendidos como uma grande coleção de pares de chave-valor, em que as chaves representam as palavras no vocabulário e os valores seus vetores correspondentes.\n",
    "\n",
    "Existe um site chamado Wikipedia2Vec ([link](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)) que compila vários embeddings pré-treinados para utilização. Vamos agora entender como construímos nosso próprio embedding usando Word2Vec.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4889d7f-968d-45df-a840-616990d0f995",
   "metadata": {},
   "source": [
    "### Construindo um Embedding usando Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce533bc-0256-4adb-b317-399ab1f5ac51",
   "metadata": {},
   "source": [
    "Baseado no artigo que originou o Word2Vec, temos duas variantes arquiteturais para treinar nosso próprio embedding: Continuous Bag of words (CBOW) e SkipGram. Ambas possuem similaridades em muitos aspectos. Vamos começar pelo CBOW:\n",
    "\n",
    "No CBOW, o objetivo é construir um modelo que corretamente prediga a palavra central dadas as palavras que estão num contexto dela. Dada uma sentença de  palavras, o modelo atribui uma probabilidade  para toda ela. Assim, o objetivo de um modelo de linguagem é atribuir probabilidades de tal maneira que forneça alta probabilidade para “boas” sentenças e baixa probabilidade para “más” sentenças. Uma “boa” sentença seria a frase: “o gato pulou sobre o cachorro”. Já uma “má” sentença seria: “sobre pulou o gato o cachorro”.\n",
    "\n",
    "Como dito, o CBOW tenta aprender um modelo de linguagem ao tentar predizer a palavra central a partir das palavras em seu contexto. Para exemplificar esse conceito, vamos usar um simples exemplo como se fosse nosso corpus: “the quick brown fox jumps over the lazy dog”.\n",
    "\n",
    "Se considerarmos a palavra “jumps” como a palavra central, então seu contexto será formado pelas palavras em sua vizinhança. Tomando uma janela de tamanho 2 para definir o contexto de uma palavra, o contexto de “jumps” seriam as palavras “brown”, “fox”, “over” e “the”.\n",
    "\n",
    "O CBOW faz isso para todas as palavras presente no corpus, ou seja, toma cada palavra no corpus como a palavra a ser predita e tenta predize-la a partir das palavras em seu contexto. A imagem abaixo nos apresenta como seria o processo de treino considerando nosso corpus de exemplo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b00f51-ea8b-4ebf-930f-d4807a34eec5",
   "metadata": {},
   "source": [
    "<img src=\"img/cbow_ideia.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e26ff-7f13-42dc-a210-ac0285548b04",
   "metadata": {},
   "source": [
    "Na prática, construímos uma rede neural que irá aprender a relação de uma palavra com as demais no corpus. Vamos ver como construir esse modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a67099-590a-4b69-a7e4-8aa78fc72187",
   "metadata": {},
   "source": [
    "<img src=\"img/cbow.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f437a-5850-4628-8f48-f578aa5c6fbc",
   "metadata": {},
   "source": [
    "Já no SkipGram, o objetivo é predizer as palavras num contexto de uma palavra central. Usando o mesmo corpus do exemplo dado no CBOW, esse seria o processo de treino para o skipgram:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedfe7d9-485f-4e60-8128-48bf96d503b2",
   "metadata": {},
   "source": [
    "<img src=\"img/skipgram_ideia.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c850393-fe86-4f0d-90b7-2e1ec735f0dc",
   "metadata": {},
   "source": [
    "A rede neural usada pelo skipgram é representada pela imagem abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa31d6d-fa09-4ca5-926b-dfa6d52a358f",
   "metadata": {},
   "source": [
    "<img src=\"img/skipgram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfd285-96e9-4b03-aa34-5c3c9e0a017c",
   "metadata": {},
   "source": [
    "Na prática, o que estamos construindo é uma rede neural com uma camada de input, uma camada intermediária e uma cada de saída (isso vale tanto para o CBOW quanto para o SkipGram, mas para efeitos didáticos, vamos tomar o SkipGram como padrão a partir de agora). Entretanto, o objetivo final não é usar essa rede treinada, mas usar apenas os pesos aprendidos na camada intermediária da rede. Vamos entender isso em detalhes.\n",
    "\n",
    "Como exemplo, considere que temos um vocabulário com 10000 palavras únicas. A camada de entrada da nossa rede será um vetor one-hot-encoding de 10000 posições (uma para cada palavra do dicionário). Supondo a palavra “ants”, será colocado 1 na posição do vetor em que essa palavra aparecer e 0 para as demais posições.\n",
    "\n",
    "A camada de saída da rede é também um vetor de 10000 posições contendo, para cada palavra no vocabulário, a probabilidade de que uma palavra próxima selecionada aleatoriamente seja aquela palavra do vocabulário. Dessa forma, podemos representar a rede neural da seguinte maneira:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a4fbe-0ee4-4728-9f49-9727043ffa5e",
   "metadata": {},
   "source": [
    "<img src=\"img/rede_skipgram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770f80d-d33f-4b31-94b8-79c93ae4c203",
   "metadata": {},
   "source": [
    "Quando falamos de Word Embeddings, dissemos que os vetores são densos e de baixa dimensionalidade. Na prática, ser de baixa dimensionalidade possui relação direta com a quantidade de neurônios na camada intermediária e esse valor é um hiperparametro que determinamos ao realizar o treinamento da rede neural. Para o nosso exemplo, vamos usar um vetor de 300 posições. Dessa maneira, a camada oculta será representada por uma matriz de pesos com 10000 linhas (tamanho do dicionário) e 300 colunas (uma para cada neurônio oculto). O objetivo, então, é aprender os pesos dessa camada oculta, que serão os vetores de embeddings das palavras no vocabulário.\n",
    "\n",
    "Você pode ter notado que nossa rede possui uma quantidade muito grande de pesos. No exemplo dado, há 3 milhões de pesos entre a camada de entrada e a camada oculta e mais 3 milhões de pesos entre a camada oculta e a camada de saída. Realizar um treino com essas especificações num dataset grande é proibitivo. Pensando nisso, os estudiosos propuseram uma maneira eficiente de realizar o treino minimizando o processamento. Essa técnica foi denominada *negative sampling*.\n",
    "\n",
    " \n",
    "Negative Sampling, então, endereça esse problema fazendo com que cada amostra de treino modifique apenas uma pequena porcentagem dos pesos. Vamos entender seu funcionamento: quando na etapa de treinamento eu tenho um par (“fox”, “quick”) a saída correta é um one-hot vector, ou seja, para o neurônio de saída que corresponde a “quick”  retornar 1 e todos os demais retornarem 0.\n",
    "\n",
    "Com o negative sampling, vamos aleatoriamente selecionar um pequeno número (5, digamos) de palavras ‘negativas’ para atualizar os pesos (‘negativa’ significa uma palavra que queremos que a rede produza um 0). Ainda atualizaremos os pesos para a palavra “positiva” (a palavra “quick” em nosso exemplo).\n",
    "Assim, vamos ajustar os pesos da palavra positiva mais os pesos de 5 palavras negativas, o que corresponde a um total de 6 neurônios e 1800 pesos no total. Isso é apenas 0,06% dos 3M de pesos da camada de saída. E como fazemos para selecionar amostras negativas?\n",
    "\n",
    "As amostras negativas (Negative Samples), ou seja, as 5 palavras que vou treinar para retornar um 0, são selecionadas usando uma distribuição unigrama, em que palavras mais frequentes são mais prováveis de serem selecionadas como amostras negativas.\n",
    "\n",
    "Supondo um vocabulário de tamanho $n$, a probabilidade de selecionar aleatoriamente uma palavra é dada pela seguinte equação:\n",
    "\n",
    "\n",
    "$p(w_i) = \\frac{f(w_i)}{\\sum_{j=0}^{n} f(w_j)}$\n",
    "\n",
    "Os autores do artigo afirmam que testaram diversas variações dessa equação e aquela que obteve o melhor resultado foi a que elevou a contagem de palavras à potência de $\\frac{3}{4}$. Assim, a equação ficaria da seguinte maneira:\n",
    "\n",
    "$p(w_i) = \\frac{f(w_i)^\\frac{3}{4}}{\\sum_{j=0}^{n} f(w_j)^\\frac{3}{4}}$\n",
    "\n",
    "O motivo é que essa maneira possui a tendência de aumentar a probabilidade de palavras menos frequentes e diminuir a probabilidade de palavras mais frequentes.\n",
    "\n",
    "Vamos agora para a parte prática, em que vamos construir nosso embedding e usá-lo para treinar um modelo de machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844e6b5-b682-46d9-a0a4-29d28272277c",
   "metadata": {},
   "source": [
    "## Prática\n",
    "\n",
    "A primeira coisa a ser feita é importar os pacotes necessários:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575e1b36-9d16-4d1a-9666-6e9e3026669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import multiprocessing\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15d082-2c79-4c1b-9187-2248714ca6b9",
   "metadata": {},
   "source": [
    "Depois, vamos ler o mesmo arquivo que usamos anteriormente, para fazermos uma comparação com os tipos de representação vistos nas aulas passadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f2f8ed-407c-4bf6-837b-1b91dcc3aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bases/uci-news-aggregator.csv')\n",
    "df = df[['TITLE','CATEGORY']]\n",
    "#categories: b = business, t = science and technology, e = entertainment, m = health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a27189-ad36-490d-9830-295f0207e895",
   "metadata": {},
   "source": [
    "Agora, precisamos embaralhar os dados. Com isso, evitamos que o modelo aprenda bem somente sobre uma classe, já que ele pode ficar preso em mínimos locais. Para isso, usaremos o método Shuffle, da biblioteca utils da Scikit-Learn. Por fim, reiniciamos o index e eliminamos a nova coluna de índice criada e mostramos as 5 primeiras linhas de nosso dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981d51a6-02e4-47b7-b9f4-b75a08c5bb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TITLE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CATEGORY",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0698ee02-e104-4e94-9aa2-454b3bb4d4a7",
       "rows": [
        [
         "0",
         "Sacci Comment On Nigerian GDP Rebasing",
         "b"
        ],
        [
         "1",
         "Fed turns hawkish or fumbles message",
         "b"
        ],
        [
         "2",
         "Gold surges as US Fed vows to stay loose",
         "b"
        ],
        [
         "3",
         "Crude & Brent Oil Weekly Fundamental Analysis June 16 – 20, 2014 Forecast",
         "b"
        ],
        [
         "4",
         "Kmart Is Selling Nicki Minaj's 'Anaconda' Shorts for Six Dollars",
         "e"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sacci Comment On Nigerian GDP Rebasing</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed turns hawkish or fumbles message</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gold surges as US Fed vows to stay loose</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crude &amp; Brent Oil Weekly Fundamental Analysis ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kmart Is Selling Nicki Minaj's 'Anaconda' Shor...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0             Sacci Comment On Nigerian GDP Rebasing        b\n",
       "1               Fed turns hawkish or fumbles message        b\n",
       "2           Gold surges as US Fed vows to stay loose        b\n",
       "3  Crude & Brent Oil Weekly Fundamental Analysis ...        b\n",
       "4  Kmart Is Selling Nicki Minaj's 'Anaconda' Shor...        e"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df)\n",
    "df = df.reset_index(drop = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050496d3-51f5-403b-9b0f-fd27162ff246",
   "metadata": {},
   "source": [
    "Vamos usar o mesmo conjunto de funções para tratamento de texto que escrevemos anteriormente. Vou colocá-lo aqui e relembrar brevemente o que cada função faz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde21188-1cfa-4771-bd1b-b45444f61377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_accents(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "def normalize_str(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = normalize_accents(text)\n",
    "    text = re.sub(re.compile(r\" +\"), \" \",text)\n",
    "    return \" \".join([w for w in text.split()])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuations = string.punctuation\n",
    "    table = str.maketrans({key: \" \" for key in punctuations})\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\") # portuguese, caso o dataset seja em português\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)\n",
    "        text = \"\".join([w for w in text if not w.isdigit()])\n",
    "        text = word_tokenize(text)\n",
    "        text = [x for x in text if x not in stop_words]\n",
    "        text = [y for y in text if len(y) > 2]\n",
    "        return [t for t in text]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb54737-8a7c-4996-9bb8-e64de6ba65f9",
   "metadata": {},
   "source": [
    "Novamente, aplicamos essas funções para tratar o texto de todas as linhas da coluna Title. O texto tratado estará dentro da nova coluna criada chamada Title_treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea57319-f7ca-49fd-bb96-7154b3d73f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_Treated'] = df['TITLE'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8580fa2b-3ad7-4ed2-83c8-50ea2bc0dc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TITLE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CATEGORY",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title_Treated",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "36fdc76d-3a4e-4a14-b69d-bb5539db75b3",
       "rows": [
        [
         "0",
         "Sacci Comment On Nigerian GDP Rebasing",
         "b",
         "['sacci', 'comment', 'nigerian', 'gdp', 'rebasing']"
        ],
        [
         "1",
         "Fed turns hawkish or fumbles message",
         "b",
         "['fed', 'turns', 'hawkish', 'fumbles', 'message']"
        ],
        [
         "2",
         "Gold surges as US Fed vows to stay loose",
         "b",
         "['gold', 'surges', 'fed', 'vows', 'stay', 'loose']"
        ],
        [
         "3",
         "Crude & Brent Oil Weekly Fundamental Analysis June 16 – 20, 2014 Forecast",
         "b",
         "['crude', 'brent', 'oil', 'weekly', 'fundamental', 'analysis', 'june', 'forecast']"
        ],
        [
         "4",
         "Kmart Is Selling Nicki Minaj's 'Anaconda' Shorts for Six Dollars",
         "e",
         "['kmart', 'selling', 'nicki', 'minaj', 'anaconda', 'shorts', 'six', 'dollars']"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>Title_Treated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sacci Comment On Nigerian GDP Rebasing</td>\n",
       "      <td>b</td>\n",
       "      <td>[sacci, comment, nigerian, gdp, rebasing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed turns hawkish or fumbles message</td>\n",
       "      <td>b</td>\n",
       "      <td>[fed, turns, hawkish, fumbles, message]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gold surges as US Fed vows to stay loose</td>\n",
       "      <td>b</td>\n",
       "      <td>[gold, surges, fed, vows, stay, loose]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crude &amp; Brent Oil Weekly Fundamental Analysis ...</td>\n",
       "      <td>b</td>\n",
       "      <td>[crude, brent, oil, weekly, fundamental, analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kmart Is Selling Nicki Minaj's 'Anaconda' Shor...</td>\n",
       "      <td>e</td>\n",
       "      <td>[kmart, selling, nicki, minaj, anaconda, short...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY  \\\n",
       "0             Sacci Comment On Nigerian GDP Rebasing        b   \n",
       "1               Fed turns hawkish or fumbles message        b   \n",
       "2           Gold surges as US Fed vows to stay loose        b   \n",
       "3  Crude & Brent Oil Weekly Fundamental Analysis ...        b   \n",
       "4  Kmart Is Selling Nicki Minaj's 'Anaconda' Shor...        e   \n",
       "\n",
       "                                       Title_Treated  \n",
       "0          [sacci, comment, nigerian, gdp, rebasing]  \n",
       "1            [fed, turns, hawkish, fumbles, message]  \n",
       "2             [gold, surges, fed, vows, stay, loose]  \n",
       "3  [crude, brent, oil, weekly, fundamental, analy...  \n",
       "4  [kmart, selling, nicki, minaj, anaconda, short...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #verificando os resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c5b36-b7c0-4dd0-aa4f-b6f53ad5a1e7",
   "metadata": {},
   "source": [
    "Agora vamos criar variáveis que serão os hiperparametros de entrada para a construção do Word2Vec usando o gensim. O gensim é uma biblioteca criada para representar documentos como um vetor semântico de maneira eficiente e menos dolorida o possível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2d825a-b963-4f92-8ee1-122fbc64304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parâmetros do word2vec\n",
    "dim_vec = 300\n",
    "min_count = 10\n",
    "window = 4\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "seed = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aafdc6-ef3b-4403-bb5c-3fca64a8a601",
   "metadata": {},
   "source": [
    "Com isso, podemos criar um modelo do Word2Vec a partir dos dados da coluna tratada. Importante notar que esse exemplo não captura tudo aquilo que o Word2Vec pode oferecer, visto que na prática treinamos com uma quantidade muito maior de texto. O objetivo aqui é apenas ilustrar o processo de treinamento de embbedings. Mesmo assim, veremos que os resultados serão muito satisfatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce9fd57-5cdc-4e45-afd8-5d8417832ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instância do Word2Vec\n",
    "modelo = Word2Vec(df[\"Title_Treated\"],\n",
    "                    min_count = min_count, \n",
    "                    vector_size = dim_vec, \n",
    "                    window = window,\n",
    "                    seed = seed,\n",
    "                    workers = num_workers,\n",
    "                    sg = 1) #sg = 0 -> CBOW e sg = 1 -> skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd792ee-13a3-4ea7-8924-0d1bc4926a9c",
   "metadata": {},
   "source": [
    "Podemos verificar o tamanho do vocabulário que o modelo criou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc65d45-d632-40cb-bea6-e9478ce74109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário do Word2Vec:  16241\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do vocabulário do Word2Vec: \", len(modelo.wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3866edd-ceba-4337-878c-c72773b5bca8",
   "metadata": {},
   "source": [
    "Treinado o modelo, conseguimos explorar um pouco as relações semânticas que ele consegue estabelecer. Veja os exemplos a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9336e118-ce36-4bbd-b49e-508d9f3d9dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('galaxy', 0.6521656513214111), ('tizen', 0.599277138710022), ('waterproof', 0.5978749990463257), ('cameraphone', 0.583593487739563), ('neo', 0.5833974480628967), ('tab', 0.5823696851730347), ('exynos', 0.5803684592247009), ('specification', 0.5749099254608154), ('htc', 0.5740392804145813), ('fingerprint', 0.5732000470161438)] \n",
      "\n",
      "0.29542693 \n",
      "\n",
      "[('film', 0.4371526539325714), ('flick', 0.39304399490356445), ('biopic', 0.3788643479347229)]\n"
     ]
    }
   ],
   "source": [
    "# exemplos das relações semânticas que o word2vec consegue estabelecer\n",
    "print(modelo.wv.most_similar('samsung'), '\\n') # palavra mais similar a 'itau'\n",
    "print(modelo.wv.similarity('google', 'microsoft'), '\\n') # similaridade entre duas palavras\n",
    "print(modelo.wv.most_similar(positive = ['show', 'movie'], negative = ['home'], topn = 3)) # similaridade considerando exemplos positivos e negativos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d873409-2b5a-4f28-8887-a1dd9aef8214",
   "metadata": {},
   "source": [
    "O Word2Vec treinado retorna um vetor de 300 dimensões para cada palavra. Entretanto, estamos trabalhando com frases. Dessa maneira, precisamos calcular o vetor das frases. Para isso, considere o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f75ecdf-904f-4017-b19f-7455a5ff8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanVector(model,phrase):\n",
    "    vocab = list(model.wv.index_to_key)\n",
    "    phrase = \" \".join(phrase)\n",
    "    phrase = [x for x in word_tokenize(phrase) if x in vocab]\n",
    "    #Quando não houver palavra o vector recebe 0 para todas as posições\n",
    "    if phrase == []:\n",
    "        vetor = [0.0]*dim_vec \n",
    "    else: \n",
    "        #Caso contrário, calculando a matriz da frase\n",
    "        vetor = np.mean([model.wv[word] for word in phrase],axis=0)\n",
    "    return vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65d035-2c8a-4381-8811-e1f7c71d2587",
   "metadata": {},
   "source": [
    "Vamos explicar a função:\n",
    "\n",
    "1. def meanVector(model, phrase): cria uma função chamada meanVector.\n",
    "\n",
    "2. vocab = list(model.wv.index_to_key): retorna uma lista com as palavras que formam o vocabulário do modelo\n",
    "\n",
    "3. phrase = “ “.join(phrase) : junta as palavras numa string só\n",
    "\n",
    "4. phrase = [x for x in word_tokenize(phrase) if x in vocab]: mantém na variável apenas palavras que estão no dicionário\n",
    "\n",
    "5. if phrase == []: vetor = [0.0]*dim_vec: se todas as palavras da frase não estiverem no dicionário, cria-se um vetor de 300 dimensoes cujos valores serão 0.0\n",
    "\n",
    "6. else: vetor = np.mean([model.wv[word] for word in phrase],axis=0): caso contrário, calcula um vetor com a média do vetor de cada palavra na frase\n",
    "\n",
    "Agora, criamos outra função que usará a função criada anteriormente para retornar as features que serão imputadas no modelo a ser treinado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154bdc83-4657-4eec-8c46-8f85cbbbec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatures(base): \n",
    "    features = [meanVector(modelo,base['Title_Treated'][i])for i in range(len(base))]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944faf0-a148-4b3e-a1b0-308af62b1839",
   "metadata": {},
   "source": [
    "Explicação da função:\n",
    "\n",
    "1. def createFeatures(base): cria uma função chamada createFeatures que recebe o dataframe como parâmetro\n",
    "\n",
    "2. calcula o vetor médio de cada frase presente na base e retorna num formato de lista de listas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d032da-e93e-4286-943c-fb6ea2b51b1b",
   "metadata": {},
   "source": [
    "Criaremos uma variável labels, que conterá os rótulos das amostras de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23681ea7-b7d3-42a2-9f5a-1998f781389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df['CATEGORY']) # label para cada uma das frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32a44b5-0d30-48a6-b02b-e289e9f68d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createFeatures(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d961a-1663-4e75-bea2-39f1c810bff2",
   "metadata": {},
   "source": [
    "Separamos os dados em conjunto de treino e teste, instanciamos e treinamos um modelo SVM, calculando o tempo de treinamento e fazemos a predicao do conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cebfcdd9-b050-4a66-b6d7-309cbffd936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.3,random_state=42)\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed4acb44-6e79-4cea-b988-9e829d0dcff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:39.237563\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "sec = end_time-start_time\n",
    "print(str(datetime.timedelta(seconds = sec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632a49f-63ab-49ea-83f5-69ce87d43019",
   "metadata": {},
   "source": [
    "Por fim, imprimimos o valor da acurácia no conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "468fc35d-fb1b-49a2-ad11-23d1f89541a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9402963874816533\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
